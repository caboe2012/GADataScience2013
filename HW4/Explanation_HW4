ABOE_HW4_LogisticRegression_Explanation

## Data Cleaning and Parsing ##
The original dataset, WHAS500.dat, is comprised of 500 data points consisting of information about 22 attributes for 500 patients who suffered a heart attack and were treated at Universtiy of Massachusettes Medical Center.  This dataset provides two columns regarding patient survival/mortality rates upon suffering the heart attack; One column for duration of hospitalization when the initial heart attack occured (denoted by dstat) and a second one indicating durations of the follow up period after the heart attack (denoteb by fstat).

Looking at the data, I wanted to see if I could find the most predicitve indicators for the fstat mortality variable.  In order to do so, it was necessary to remove the 39 rows of patients who did not survive the initial heart attack (those with a dstat equal to 1).  This left 461 patients to analyze.  Given that 461 is a prime number, I decided to randomly remove one additional row and consider this part of my model's error so that I could more readily complete crossvaliation analysis on the data.

## Feature Selection and Linear Regression ##
Once, I had my dataset completed, I set about determing which of of the 22 attributes were the most predictive.In order to do so, I first conducted some basic on-line research about each attribute to better understand the meaning and implication of each for heart attack patients.  After this research, I narrowed the 22 attributes provided down to the 9 attributes listed in the below linear regression.

lin.fit <- lm(fstat ~ 0 + age + gender + bmi + cvd + sho + chf + av3 + miord + lenfol, data=x)

Running this model returned an R-squared of .4116, which was too low to be considered useful.  However, in order to see if I could improve my model, I set out removing covariates with abs t-values < 1.  This led to removing the following varialbes in the order indicated:

1) gender
2) cvd
3) sho
4) av3

With these covariates removed, my updated model obtained an R-squared of 0.6356, which seemed like it might prove fairly predictive.  This final model is listed below.

lin.fit5 <- lm(fstat ~ 0 + age + bmi + chf + miord + lenfol, data=x)

with covariates:
1) Age (at time of hospitalization)
2) bmi (body mass index)
3) chf (history of congestive heart complications)
4) miord (Heart attack order, first = 0, not first=1)
5) lenfol (Length of follow-up from time of hospitalization)

## Scatter and qqnorm plots ##
To check my analysis further, I graphed the residuals in a scatter plot and found them to appear random.  I then did a qqnorm plot of the residuals and found them to form a nice diagnoal as well.  

## Logistic Regression ##
Based on the success of the model, analyses and graphs, I decided to see if could further improve the model by converting it to a logistic regression (shown below)

logit.fit <- glm(fstat ~ age + bmi + chf + miord + lenfol, family='binomial', data=train.data)

## Generalization Error Rate ##
This model proved fairly predictive, obtaining an average Generalization Error rate of 17.39% when run against the randmoized test crossvalidation splits (ranging as low as 13% and as high as 28% depending on the split).  

## Survival Probablity Graphs (Survival and KMSurv packages) ##
To represent the data visually, I used the Surv and survfit functions from the Survival and KMsurv libraries in order to graph the Probability of Survival versus the duration of the Length of Follow-Up (lenfol) for each patient in both the Training and Test splits.

## Observations ##
While I am pleased with the predictive ability of the model, it certainly could be improved.  This could be done with additional data points that the model has not seen yet in order to more thoroughly test the assumptions.  A seond improvement would be to find a means of more appropriately accounting for the age of the patients.  In some cases, patients are extremely elderly (as old as 104) so Age may be more of a factor than the fact that the patient simply suffered a heart attack.  A dataset consisting of patients exclusively below a certain age (say 55 or 60) would perhaps be best for such an analysis.

## Implications and Business Opportunities ##
Such analyses could prove valuable to pharmaceutical companies developing drugs that can help reduce risk of recurrence of heart attacks in patients.  In this case, an analysis of the increase of the lenfol variable measured against a decling rate of fstat mortality would be the ideal KPI.







